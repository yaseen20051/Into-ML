{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "febeabd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node and DecisionTree classes (copied from BreastCancer.ipynb for Random Forest use)\n",
    "class Node():\n",
    "    def __init__(self,featureIndex = None ,threshold = None,left = None ,right = None,infoGain = None ,value = None ):\n",
    "        self.featureIndex = featureIndex\n",
    "        self.threshold = threshold \n",
    "        self.left = left \n",
    "        self.right = right \n",
    "        self.infoGain = infoGain\n",
    "        self.value = value \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class DecisionTree():\n",
    "    def __init__(self, minSamples, maxDepth, class_names=None):\n",
    "        self.minSamples = minSamples\n",
    "        self.maxDepth = maxDepth\n",
    "        self.root = None\n",
    "        self.class_names = class_names\n",
    "\n",
    "    def buildTree(self, dataset, currDepth=0):\n",
    "        # dataset is expected to be a pandas DataFrame where last column is the target\n",
    "        X, Y = dataset.iloc[:, :-1], dataset.iloc[:, -1]\n",
    "        numSamples, numCols = dataset.shape\n",
    "        numFeatures = numCols - 1\n",
    "        # leaf node \n",
    "        if numSamples <= self.minSamples or currDepth >= self.maxDepth:\n",
    "            leafValue = self.calcLeafValue(Y)\n",
    "            return Node(value=leafValue)\n",
    "        bestSplit = self.getBestSplit(dataset=dataset, numSamples=numSamples, numFeatures=numFeatures)\n",
    "        # if no valid split found, make a leaf\n",
    "        if not bestSplit or bestSplit.get(\"infoGain\", 0) <= 0:\n",
    "            leafValue = self.calcLeafValue(Y)\n",
    "            return Node(value=leafValue)\n",
    "        #  build subtrees\n",
    "        left_subtree = self.buildTree(bestSplit[\"left\"], currDepth + 1)\n",
    "        right_subtree = self.buildTree(bestSplit[\"right\"], currDepth + 1)\n",
    "        return Node(featureIndex=bestSplit[\"featureIndex\"], threshold=bestSplit[\"threshold\"], left=left_subtree, right=right_subtree, infoGain=bestSplit[\"infoGain\"])\n",
    "\n",
    "    def getBestSplit(self, dataset, numSamples, numFeatures):\n",
    "        max_IG = -float(\"inf\")\n",
    "        bestSplit = {}\n",
    "        for featureIndex in range(numFeatures):\n",
    "            featureSamples = dataset.iloc[:, featureIndex]\n",
    "            uniqueThresholds = np.sort(np.unique(featureSamples))\n",
    "            if len(uniqueThresholds) <= 1:\n",
    "                continue\n",
    "            possibleThresholds = (uniqueThresholds[:-1] + uniqueThresholds[1:]) / 2.0\n",
    "            for threshold in possibleThresholds:\n",
    "                leftDataset, rightDataset = self.split(dataset=dataset, featureIndex=featureIndex, threshold=threshold)\n",
    "                if len(leftDataset) > 0 and len(rightDataset) > 0:\n",
    "                    datasetTargets = dataset.iloc[:, -1]\n",
    "                    leftTargets = leftDataset.iloc[:, -1]\n",
    "                    rightTargets = rightDataset.iloc[:, -1]\n",
    "                    datasetEntropy = self.entropy(datasetTargets)\n",
    "                    leftDatasetEntropy = self.entropy(leftTargets)\n",
    "                    rightDatasetEntropy = self.entropy(rightTargets)\n",
    "                    IG = self.informationGain(datasetEntropy, leftDatasetEntropy, rightDatasetEntropy, datasetTargets, leftTargets, rightTargets)\n",
    "                    if IG > max_IG:\n",
    "                        max_IG = IG\n",
    "                        bestSplit[\"featureIndex\"] = featureIndex\n",
    "                        bestSplit[\"threshold\"] = threshold\n",
    "                        bestSplit[\"left\"] = leftDataset\n",
    "                        bestSplit[\"right\"] = rightDataset\n",
    "                        bestSplit[\"infoGain\"] = IG\n",
    "        return bestSplit\n",
    "\n",
    "    def split(self, dataset, featureIndex, threshold):\n",
    "        mask = np.array(dataset.iloc[:, featureIndex]) <= threshold\n",
    "        leftDataset, rightDataset = dataset[mask], dataset[~mask]\n",
    "        return leftDataset, rightDataset\n",
    "\n",
    "    def entropy(self, dataset):\n",
    "        datasetEntropy = 0.0\n",
    "        numberOfExamples = len(dataset)\n",
    "        classesLabels = np.unique(dataset)\n",
    "        for cls in classesLabels:\n",
    "            p = len(dataset[dataset == cls]) / numberOfExamples\n",
    "            if p > 0:\n",
    "                datasetEntropy = datasetEntropy - (p * np.log2(p))\n",
    "        return datasetEntropy\n",
    "\n",
    "    def informationGain(self, datasetEntropy, leftDatasetEntropy, rightDatasetEntropy, datasetTargets, leftTargets, rightTargets):\n",
    "        l_weight = len(leftTargets) / len(datasetTargets)\n",
    "        r_weight = len(rightTargets) / len(datasetTargets)\n",
    "        IG = datasetEntropy - (l_weight * leftDatasetEntropy + r_weight * rightDatasetEntropy)\n",
    "        return IG\n",
    "\n",
    "    def calcLeafValue(self, Y):\n",
    "        # Y may be a pandas Series; use numpy unique for counts\n",
    "        classes, counts = np.unique(Y, return_counts=True)\n",
    "        return classes[np.argmax(counts)]\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        # Ensure dataset is a pandas DataFrame so .iloc works in buildTree\n",
    "        # X and Y may be numpy arrays; concatenate along columns and wrap in DataFrame\n",
    "        dataset = pd.DataFrame(np.concatenate((X, Y), axis=1))\n",
    "        self.root = self.buildTree(dataset)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = [self.makePrediction(x, self.root) for x in X]\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def makePrediction(self, x, tree):\n",
    "        if tree.value is not None:\n",
    "            return tree.value\n",
    "        feature = x[tree.featureIndex]\n",
    "        if feature <= tree.threshold:\n",
    "            return self.makePrediction(x, tree.left)\n",
    "        else:\n",
    "            return self.makePrediction(x, tree.right)\n",
    "\n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "        if tree.value is not None:\n",
    "            print(tree.value)\n",
    "        else:\n",
    "            print(\"X_\" + str(tree.featureIndex), \"<=\", tree.threshold, \"?\", tree.infoGain)\n",
    "            print(\"%sleft:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.left, indent + indent)\n",
    "            print(\"%sright:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.right, indent + indent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2bc174",
   "metadata": {},
   "source": [
    "## Custom Random Forest Implementation\n",
    "\n",
    "This section implements a Random Forest from scratch using your own DecisionTree class. Each tree is trained on a bootstrap sample, and at each split, a random subset of features is considered. The final prediction is made by majority voting across all trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7949febe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Random Forest (using your DecisionTree)\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_trees=10, max_features=None, max_depth=None, min_samples_split=2, class_names=None, random_state=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_features = max_features\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.class_names = class_names\n",
    "        self.trees = []\n",
    "        self.random_state = random_state\n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "            random.seed(random_state)\n",
    "\n",
    "    def _bootstrap_sample(self, X, Y):\n",
    "        n_samples = X.shape[0]\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        return X[indices], Y[indices]\n",
    "\n",
    "    def _get_feature_indices(self, n_features):\n",
    "        if self.max_features is None:\n",
    "            return np.arange(n_features)\n",
    "        else:\n",
    "            return np.random.choice(n_features, self.max_features, replace=False)\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.trees = []\n",
    "        n_features = X.shape[1]\n",
    "        for i in range(self.n_trees):\n",
    "            X_sample, Y_sample = self._bootstrap_sample(X, Y)\n",
    "            feature_indices = self._get_feature_indices(n_features)\n",
    "            tree = DecisionTree(\n",
    "                minSamples=self.min_samples_split,\n",
    "                maxDepth=self.max_depth,\n",
    "                class_names=self.class_names\n",
    "            )\n",
    "            # Only use selected features for this tree\n",
    "            X_tree = X_sample[:, feature_indices]\n",
    "            tree.fit(X_tree, Y_sample)\n",
    "            self.trees.append((tree, feature_indices))\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Each tree predicts using its own feature subset\n",
    "        tree_preds = []\n",
    "        for tree, feature_indices in self.trees:\n",
    "            X_sub = X[:, feature_indices]\n",
    "            preds = tree.predict(X_sub)\n",
    "            tree_preds.append(preds)\n",
    "        # Majority vote\n",
    "        tree_preds = np.array(tree_preds)  # shape: (n_trees, n_samples)\n",
    "        y_pred = []\n",
    "        for i in range(X.shape[0]):\n",
    "            votes = tree_preds[:, i]\n",
    "            # If class_names is set, map back to numeric for voting\n",
    "            if self.class_names is not None and isinstance(self.class_names, (list, tuple)):\n",
    "                # Map class names to indices for voting\n",
    "                votes = [self.class_names.index(v) if v in self.class_names else v for v in votes]\n",
    "            vote = Counter(votes).most_common(1)[0][0]\n",
    "            # If class_names, map back to name\n",
    "            if self.class_names is not None and isinstance(self.class_names, (list, tuple)):\n",
    "                vote = self.class_names[int(vote)]\n",
    "            y_pred.append(vote)\n",
    "        return np.array(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2616100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (397, 30), Val: (86, 30), Test: (86, 30)\n",
      "T=5, max_features=5: Val Accuracy=0.0000\n",
      "T=5, max_features=5: Val Accuracy=0.0000\n",
      "T=5, max_features=15: Val Accuracy=0.0000\n",
      "T=5, max_features=15: Val Accuracy=0.0000\n",
      "T=10, max_features=5: Val Accuracy=0.0000\n",
      "T=10, max_features=5: Val Accuracy=0.0000\n",
      "T=10, max_features=15: Val Accuracy=0.0000\n",
      "T=10, max_features=15: Val Accuracy=0.0000\n",
      "T=30, max_features=5: Val Accuracy=0.0000\n",
      "T=30, max_features=5: Val Accuracy=0.0000\n",
      "T=30, max_features=15: Val Accuracy=0.0000\n",
      "T=30, max_features=15: Val Accuracy=0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 40\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m max_feat \u001b[38;5;129;01min\u001b[39;00m grid_max_features:\n\u001b[1;32m     32\u001b[0m     rf \u001b[38;5;241m=\u001b[39m RandomForest(\n\u001b[1;32m     33\u001b[0m         n_trees\u001b[38;5;241m=\u001b[39mT,\n\u001b[1;32m     34\u001b[0m         max_features\u001b[38;5;241m=\u001b[39mmax_feat,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m         random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     39\u001b[0m     )\n\u001b[0;32m---> 40\u001b[0m     rf\u001b[38;5;241m.\u001b[39mfit(X_train,Y_train)\n\u001b[1;32m     41\u001b[0m     Y_val_pred \u001b[38;5;241m=\u001b[39m rf\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[1;32m     42\u001b[0m     acc \u001b[38;5;241m=\u001b[39m accuracy_score(Y_val, Y_val_pred)\n",
      "Cell \u001b[0;32mIn[10], line 43\u001b[0m, in \u001b[0;36mRandomForest.fit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Only use selected features for this tree\u001b[39;00m\n\u001b[1;32m     42\u001b[0m X_tree \u001b[38;5;241m=\u001b[39m X_sample[:, feature_indices]\n\u001b[0;32m---> 43\u001b[0m tree\u001b[38;5;241m.\u001b[39mfit(X_tree, Y_sample)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrees\u001b[38;5;241m.\u001b[39mappend((tree, feature_indices))\n",
      "Cell \u001b[0;32mIn[7], line 98\u001b[0m, in \u001b[0;36mDecisionTree.fit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, Y):\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# Ensure dataset is a pandas DataFrame so .iloc works in buildTree\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# X and Y may be numpy arrays; concatenate along columns and wrap in DataFrame\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(np\u001b[38;5;241m.\u001b[39mconcatenate((X, Y), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuildTree(dataset)\n",
      "Cell \u001b[0;32mIn[7], line 36\u001b[0m, in \u001b[0;36mDecisionTree.buildTree\u001b[0;34m(self, dataset, currDepth)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Node(value\u001b[38;5;241m=\u001b[39mleafValue)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#  build subtrees\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m left_subtree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuildTree(bestSplit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m], currDepth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     37\u001b[0m right_subtree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuildTree(bestSplit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m], currDepth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Node(featureIndex\u001b[38;5;241m=\u001b[39mbestSplit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatureIndex\u001b[39m\u001b[38;5;124m\"\u001b[39m], threshold\u001b[38;5;241m=\u001b[39mbestSplit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m\"\u001b[39m], left\u001b[38;5;241m=\u001b[39mleft_subtree, right\u001b[38;5;241m=\u001b[39mright_subtree, infoGain\u001b[38;5;241m=\u001b[39mbestSplit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfoGain\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[7], line 36\u001b[0m, in \u001b[0;36mDecisionTree.buildTree\u001b[0;34m(self, dataset, currDepth)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Node(value\u001b[38;5;241m=\u001b[39mleafValue)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#  build subtrees\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m left_subtree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuildTree(bestSplit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m], currDepth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     37\u001b[0m right_subtree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuildTree(bestSplit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m], currDepth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Node(featureIndex\u001b[38;5;241m=\u001b[39mbestSplit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatureIndex\u001b[39m\u001b[38;5;124m\"\u001b[39m], threshold\u001b[38;5;241m=\u001b[39mbestSplit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m\"\u001b[39m], left\u001b[38;5;241m=\u001b[39mleft_subtree, right\u001b[38;5;241m=\u001b[39mright_subtree, infoGain\u001b[38;5;241m=\u001b[39mbestSplit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfoGain\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[7], line 30\u001b[0m, in \u001b[0;36mDecisionTree.buildTree\u001b[0;34m(self, dataset, currDepth)\u001b[0m\n\u001b[1;32m     28\u001b[0m     leafValue \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalcLeafValue(Y)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Node(value\u001b[38;5;241m=\u001b[39mleafValue)\n\u001b[0;32m---> 30\u001b[0m bestSplit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetBestSplit(dataset\u001b[38;5;241m=\u001b[39mdataset, numSamples\u001b[38;5;241m=\u001b[39mnumSamples, numFeatures\u001b[38;5;241m=\u001b[39mnumFeatures)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# if no valid split found, make a leaf\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m bestSplit \u001b[38;5;129;01mor\u001b[39;00m bestSplit\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfoGain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[7], line 56\u001b[0m, in \u001b[0;36mDecisionTree.getBestSplit\u001b[0;34m(self, dataset, numSamples, numFeatures)\u001b[0m\n\u001b[1;32m     54\u001b[0m rightTargets \u001b[38;5;241m=\u001b[39m rightDataset\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     55\u001b[0m datasetEntropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentropy(datasetTargets)\n\u001b[0;32m---> 56\u001b[0m leftDatasetEntropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentropy(leftTargets)\n\u001b[1;32m     57\u001b[0m rightDatasetEntropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentropy(rightTargets)\n\u001b[1;32m     58\u001b[0m IG \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minformationGain(datasetEntropy, leftDatasetEntropy, rightDatasetEntropy, datasetTargets, leftTargets, rightTargets)\n",
      "Cell \u001b[0;32mIn[7], line 80\u001b[0m, in \u001b[0;36mDecisionTree.entropy\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     78\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset[dataset \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mcls\u001b[39m]) \u001b[38;5;241m/\u001b[39m numberOfExamples\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 80\u001b[0m         datasetEntropy \u001b[38;5;241m=\u001b[39m datasetEntropy \u001b[38;5;241m-\u001b[39m (p \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog2(p))\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m datasetEntropy\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# D3. Hyperparameter Tuning and Evaluation for Custom Random Forest\n",
    "from math import sqrt, floor\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Load data (Breast Cancer Wisconsin)\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "Y = data.target.reshape(-1, 1)\n",
    "feature_names = data.feature_names\n",
    "class_names = list(data.target_names)\n",
    "\n",
    "# 70/15/15 split (train/val/test)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_trainval, X_test, Y_trainval, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42, stratify=Y)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_trainval, Y_trainval, test_size=0.1765, random_state=42, stratify=Y_trainval)\n",
    "# 0.1765 â‰ˆ 15/(85) to get 70/15/15\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# Use best max_depth and min_samples_split from Part B (replace with your actual values)\n",
    "best_max_depth = 4  # <-- set this to your best value from Part B\n",
    "best_min_samples = 2  # <-- set this to your best value from Part B\n",
    "\n",
    "d = X.shape[1]\n",
    "grid_T = [5, 10, 30, 50]\n",
    "grid_max_features = [floor(sqrt(d)), floor(d/2)]\n",
    "\n",
    "results = []\n",
    "for T in grid_T:\n",
    "    for max_feat in grid_max_features:\n",
    "        rf = RandomForest(\n",
    "            n_trees=T,\n",
    "            max_features=max_feat,\n",
    "            max_depth=best_max_depth,\n",
    "            min_samples_split=best_min_samples,\n",
    "            class_names=class_names,\n",
    "            random_state=42\n",
    "        )\n",
    "        rf.fit(X_train,Y_train)\n",
    "        Y_val_pred = rf.predict(X_val)\n",
    "        acc = accuracy_score(Y_val, Y_val_pred)\n",
    "        results.append({\n",
    "            'T': T,\n",
    "            'max_features': max_feat,\n",
    "            'val_accuracy': acc\n",
    "        })\n",
    "        print(f\"T={T}, max_features={max_feat}: Val Accuracy={acc:.4f}\")\n",
    "\n",
    "# Select best hyperparameters\n",
    "results_df = pd.DataFrame(results)\n",
    "best_idx = results_df['val_accuracy'].idxmax()\n",
    "best_params = results_df.iloc[best_idx]\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "print(best_params)\n",
    "\n",
    "# Retrain on train+val with best params, evaluate on test set\n",
    "rf_final = RandomForest(\n",
    "    n_trees=int(best_params['T']),\n",
    "    max_features=int(best_params['max_features']),\n",
    "    max_depth=best_max_depth,\n",
    "    min_samples_split=best_min_samples,\n",
    "    class_names=class_names,\n",
    "    random_state=42\n",
    ")\n",
    "rf_final.fit(np.vstack([X_train, X_val]), np.vstack([Y_train, Y_val]))\n",
    "Y_test_pred = rf_final.predict(X_test)\n",
    "test_acc = accuracy_score(Y_test, Y_test_pred)\n",
    "print(f\"\\nTest Accuracy (Random Forest): {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec30bc3",
   "metadata": {},
   "source": [
    "## Random Forest Performance Analysis\n",
    "\n",
    "Evaluate the trained Random Forest on the test set: accuracy, precision, recall, F1-score, confusion matrix, and feature importance. Compare with the single Decision Tree results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd63721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RANDOM FOREST PERFORMANCE ON TEST SET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(Y_test, Y_test_pred)\n",
    "precision = precision_score(Y_test, Y_test_pred, average=None)\n",
    "recall = recall_score(Y_test, Y_test_pred, average=None)\n",
    "f1 = f1_score(Y_test, Y_test_pred, average=None)\n",
    "\n",
    "print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nPer-Class Metrics:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f\"\\n{class_name.upper()}:\")\n",
    "    print(f\"  - Precision: {precision[i]:.4f}\")\n",
    "    print(f\"  - Recall: {recall[i]:.4f}\")\n",
    "    print(f\"  - F1-Score: {f1[i]:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Classification Report\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(Y_test, Y_test_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90376d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(Y_test, Y_test_pred)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Random Forest Confusion Matrix (Test Set)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print('Confusion matrix:')\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f24fdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance (Mean Decrease in Impurity)\n",
    "importances = np.zeros(X.shape[1])\n",
    "for tree, feat_idx in rf_final.trees:\n",
    "    # Each tree was trained on a subset of features\n",
    "    # We'll sum the importance for the selected features\n",
    "    if hasattr(tree, 'root'):\n",
    "        # Traverse tree to sum infoGain per feature\n",
    "        def traverse(node, feat_map):\n",
    "            if node is None or node.value is not None:\n",
    "                return\n",
    "            feat_map[feat_idx[node.featureIndex]] += node.infoGain if node.infoGain is not None else 0\n",
    "            traverse(node.left, feat_map)\n",
    "            traverse(node.right, feat_map)\n",
    "        traverse(tree.root, importances)\n",
    "\n",
    "# Normalize importances\n",
    "importances = importances / importances.sum()\n",
    "\n",
    "# Plot\n",
    "indices = np.argsort(importances)[::-1]\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Random Forest Feature Importances (Custom)')\n",
    "plt.bar(range(X.shape[1]), importances[indices], align='center')\n",
    "plt.xticks(range(X.shape[1]), [feature_names[i] for i in indices], rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Top 10 features:')\n",
    "for i in indices[:10]:\n",
    "    print(f'{feature_names[i]}: {importances[i]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cde737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree Complexity Analysis (Average Depth, Node Count)\n",
    "total_nodes = 0\n",
    "total_leaves = 0\n",
    "total_depth = 0\n",
    "for tree, feat_idx in rf_final.trees:\n",
    "    def count_nodes(node):\n",
    "        if node is None:\n",
    "            return 0\n",
    "        return 1 + count_nodes(node.left) + count_nodes(node.right)\n",
    "    def count_leaves(node):\n",
    "        if node is None:\n",
    "            return 0\n",
    "        if node.value is not None:\n",
    "            return 1\n",
    "        return count_leaves(node.left) + count_leaves(node.right)\n",
    "    def get_depth(node):\n",
    "        if node is None or node.value is not None:\n",
    "            return 0\n",
    "        return 1 + max(get_depth(node.left), get_depth(node.right))\n",
    "    total_nodes += count_nodes(tree.root)\n",
    "    total_leaves += count_leaves(tree.root)\n",
    "    total_depth += get_depth(tree.root)\n",
    "\n",
    "n_trees = len(rf_final.trees)\n",
    "avg_nodes = total_nodes / n_trees\n",
    "avg_leaves = total_leaves / n_trees\n",
    "avg_depth = total_depth / n_trees\n",
    "\n",
    "print(f\"Random Forest (Custom) - Average per tree:\")\n",
    "print(f\"  - Nodes: {avg_nodes:.1f}\")\n",
    "print(f\"  - Leaves: {avg_leaves:.1f}\")\n",
    "print(f\"  - Depth: {avg_depth:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15de216",
   "metadata": {},
   "source": [
    "## Comparison with Single Decision Tree\n",
    "\n",
    "Compare the test set performance, bias, and variance of your best single Decision Tree (from Part C) and your custom Random Forest (from Part D). Discuss the effect of ensembling on bias and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2946c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) If you have test results from your best Decision Tree, enter them here for comparison\n",
    "# Example: dt_test_acc = 0.93\n",
    "# dt_test_precision = ...\n",
    "# dt_test_recall = ...\n",
    "# dt_test_f1 = ...\n",
    "\n",
    "# Print comparison table\n",
    "print(\"Comparison of Single Decision Tree vs Random Forest (Test Set):\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Model':<20}{'Accuracy':<12}{'Precision':<12}{'Recall':<12}{'F1':<12}\")\n",
    "print(\"-\"*60)\n",
    "# Uncomment and fill in your Decision Tree results:\n",
    "# print(f\"{'Decision Tree':<20}{dt_test_acc:<12.4f}{dt_test_precision:<12.4f}{dt_test_recall:<12.4f}{dt_test_f1:<12.4f}\")\n",
    "print(f\"{'Random Forest':<20}{accuracy:<12.4f}{precision.mean():<12.4f}{recall.mean():<12.4f}{f1.mean():<12.4f}\")\n",
    "\n",
    "print(\"\\nDiscussion:\")\n",
    "print(\"- Random Forest typically reduces variance (less overfitting) and may slightly increase bias compared to a single tree.\")\n",
    "print(\"- In most cases, ensemble methods like Random Forest achieve higher test accuracy and more stable predictions.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
